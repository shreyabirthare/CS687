{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_STATE = (0,0)\n",
    "FOOD_STATE = (4,4)\n",
    "ACTIONS = [(0,-1), (0,1), (-1,0), (1,0)]\n",
    "p={}\n",
    "p['specified'] = 0.7\n",
    "p['right'] = 0.12\n",
    "p['left'] = 0.12\n",
    "p['sleepy'] = 0.06\n",
    "FORBIDDEN_FURNITURES = [(2,1), (2,2), (2,3), (3,2)]\n",
    "MONSTERS = [(0,3), (4,1)]\n",
    "DELTA = 0.1\n",
    "GAMMA = 0.925"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTransitionProbabilities(action, p):\n",
    "    if action == (0, -1):\n",
    "        specified = (0, -1)\n",
    "        left = (1, 0)\n",
    "        right = (-1, 0)\n",
    "    elif action == (0, 1):\n",
    "        specified = (0, 1)\n",
    "        left = (-1, 0)\n",
    "        right = (1, 0)\n",
    "    elif action == (-1, 0):\n",
    "        specified = (-1, 0)\n",
    "        left = (0, -1)\n",
    "        right = (0, 1)\n",
    "    else:\n",
    "        specified = (1, 0)\n",
    "        left = (0, 1)\n",
    "        right = (0, -1)\n",
    "\n",
    "    sleepy = (0, 0)\n",
    "    return [(specified, p['specified']), (left, p['left']), (right, p['right']), (sleepy, p['sleepy'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getReward(state):\n",
    "    if state == FOOD_STATE:\n",
    "        return 10\n",
    "    elif state in MONSTERS:\n",
    "        return -8\n",
    "    else:\n",
    "        return -0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isValidState(i, j):\n",
    "    if i < 0 or j < 0 or i >= 5 or j >= 5:\n",
    "        return False\n",
    "    if (i, j) in FORBIDDEN_FURNITURES:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_initial_state():\n",
    "    while True:\n",
    "        state = (np.random.randint(5), np.random.randint(5))\n",
    "        if state not in FORBIDDEN_FURNITURES:\n",
    "            return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(policy_mat):\n",
    "    episode = []\n",
    "    state = random_initial_state()\n",
    "    while state != FOOD_STATE:\n",
    "        action = policy_mat[state[0]][state[1]]\n",
    "        transitions = getTransitionProbabilities(action, p)\n",
    "        \n",
    "        next_state = (-1, -1)\n",
    "        moves, probs = zip(*transitions)\n",
    "        chosen_move = np.random.choice(len(moves), p=probs)\n",
    "        move = moves[chosen_move]\n",
    "        next_state = (state[0] + move[0], state[1] + move[1])\n",
    "        if not isValidState(*next_state):\n",
    "            next_state = state\n",
    "\n",
    "        \n",
    "        reward = getReward(next_state)\n",
    "        episode.append((state, reward))\n",
    "        state = next_state\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_visit_monte_carlo(policy_mat, value_iteration_v):\n",
    "    V = np.zeros((5, 5))\n",
    "    visit_counts = {state: 0 for state in [(i, j) for i in range(5) for j in range(5) if (i, j) not in FORBIDDEN_FURNITURES]}\n",
    "    max_norm = float('inf')\n",
    "    trajectory_count = 0\n",
    "\n",
    "    while max_norm > DELTA:\n",
    "        episode = generate_episode(policy_mat)\n",
    "        trajectory_count += 1\n",
    "        visited_states = set()\n",
    "\n",
    "        for t, (state, _) in enumerate(episode):\n",
    "            if state not in visited_states:\n",
    "                visited_states.add(state)\n",
    "                G = 0\n",
    "                for i, (_, reward) in enumerate(episode[t:]):\n",
    "                    G += (GAMMA ** i) * reward\n",
    "\n",
    "                visit_counts[state] += 1\n",
    "                n = visit_counts[state]\n",
    "                V[state[0], state[1]] = V[state[0], state[1]] + (G - V[state[0], state[1]]) / n\n",
    "\n",
    "        max_norm = np.max(np.abs(V - value_iteration_v))\n",
    "\n",
    "    return V, trajectory_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def every_visit_monte_carlo(policy_mat, value_iteration_v):\n",
    "    V = np.zeros((5, 5))\n",
    "    visit_counts = {state: 0 for state in [(i, j) for i in range(5) for j in range(5) if (i, j) not in FORBIDDEN_FURNITURES]}\n",
    "    max_norm = float('inf')\n",
    "    trajectory_count = 0\n",
    "\n",
    "    while max_norm > DELTA:\n",
    "        episode = generate_episode(policy_mat)\n",
    "        trajectory_count += 1\n",
    "\n",
    "        G = 0\n",
    "        for t in reversed(range(len(episode))):\n",
    "            state, reward = episode[t]\n",
    "            G = reward + GAMMA * G\n",
    "\n",
    "            visit_counts[state] += 1\n",
    "            n = visit_counts[state]\n",
    "            V[state[0], state[1]] = V[state[0], state[1]] + (G - V[state[0], state[1]]) / n\n",
    "\n",
    "        max_norm = np.max(np.abs(V - value_iteration_v))\n",
    "\n",
    "    return V, trajectory_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_mat = [\n",
    "    [(0, 1), (1, 0), (0, -1), (1, 0), (1, 0)],\n",
    "    [(0, 1), (0, 1), (0, 1), (0, 1), (1, 0)],   \n",
    "    [(-1, 0), None, None, None, (1, 0)],\n",
    "    [(-1, 0), (0, -1), None, (0, 1), (1, 0)],\n",
    "    [(-1, 0), (0, 1), (0, 1), (0, 1), None]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_iteration_v = np.array([\n",
    "    [2.6638, 2.9969, 2.8117, 3.6671, 4.8497],\n",
    "    [2.9713, 3.5101, 4.0819, 4.8497, 7.1648],\n",
    "    [2.5936, 0.0,    0.0,    0.0,    8.4687],\n",
    "    [2.0992, 1.0849, 0.0,    8.6097, 9.5269],\n",
    "    [1.0849, 4.9465, 8.4687, 9.5269, 0.0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Value Function using First-Visit Monte Carlo:\n",
      " [[2.64696838 2.98428491 2.75836578 3.56853442 4.85895907]\n",
      " [2.95802445 3.48637308 4.07076583 4.80205315 7.13297904]\n",
      " [2.58369027 0.         0.         0.         8.46087393]\n",
      " [2.05684464 1.17865945 0.         8.6272001  9.5266583 ]\n",
      " [1.1129969  4.95736015 8.48280033 9.54740927 0.        ]]\n",
      "\n",
      "Total trajectories needed for convergence (max-norm < 0.1): 15313\n"
     ]
    }
   ],
   "source": [
    "V_mc, num_trajectories = first_visit_monte_carlo(policy_mat, value_iteration_v)\n",
    "print(\"Estimated Value Function using First-Visit Monte Carlo:\\n\", V_mc)\n",
    "print(\"\\nTotal trajectories needed for convergence (max-norm < 0.1):\", num_trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Value Function using Every-Visit Monte Carlo:\n",
      " [[2.56451236 2.97033026 2.90205953 3.75829544 4.81889039]\n",
      " [3.01940378 3.51283498 4.09143634 4.87048023 7.15409821]\n",
      " [2.63731905 0.         0.         0.         8.46279127]\n",
      " [2.11932044 1.1074043  0.         8.60347848 9.52270901]\n",
      " [1.03631565 4.88847717 8.50112632 9.53591615 0.        ]]\n",
      "\n",
      "Total trajectories needed for convergence (max-norm < 0.1): 14852\n"
     ]
    }
   ],
   "source": [
    "V_mc, num_trajectories = every_visit_monte_carlo(policy_mat, value_iteration_v)\n",
    "print(\"Estimated Value Function using Every-Visit Monte Carlo:\\n\", V_mc)\n",
    "print(\"\\nTotal trajectories needed for convergence (max-norm < 0.1):\", num_trajectories)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
